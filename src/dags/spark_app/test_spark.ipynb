{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc9f9af0-3fa0-441d-a7e4-cba711c56d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd\n",
    "# Minio\n",
    "from minio import Minio\n",
    "from io import BytesIO\n",
    "from minio.error import S3Error\n",
    "# Utility\n",
    "import json, sys, os, datetime\n",
    "from gecko_transform import *\n",
    "from create_date_dim import create_date_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7aac9f8-b82a-44ad-9cf8-d9e9504d63ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Spark session created !\n"
     ]
    }
   ],
   "source": [
    "def connect_to_minio():\n",
    "    access_key = 'vp821YRUCKiTgGoMEjR6'\n",
    "    secret_key = 'TkrExNxV2ozU3l9O59nk561fCSONQZSWvrZKoOpK'\n",
    "    minio_client = Minio(\n",
    "        endpoint='localhost:9000',  # Use the HTTP port\n",
    "        access_key=access_key,\n",
    "        secret_key=secret_key,\n",
    "        secure=False  # Ensure this is False for HTTP\n",
    "    )\n",
    "    return minio_client\n",
    "\n",
    "def create_spark_session():\n",
    "    # Create a SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Spark with Minio\") \\\n",
    "        .config(\"spark.log.level\", \"WARN\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.1\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", \"my_account\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", \"123456789\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    spark.conf.set('spark.sql.caseSensitive', True)\n",
    "    print(\"> Spark session created !\")\n",
    "    return spark\n",
    "\n",
    "minio_client = connect_to_minio()\n",
    "spark = create_spark_session()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "714d8ca9-2d03-492d-ad73-37213a10a47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define schema\n",
    "bars_schema = StructType([\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"open_time\", LongType(), True),\n",
    "    StructField(\"symbol_name\", StringType(), True),\n",
    "    StructField(\"open\", DoubleType(), True),\n",
    "    StructField(\"high\", DoubleType(), True),\n",
    "    StructField(\"low\", DoubleType(), True),\n",
    "    StructField(\"close\", DoubleType(), True),\n",
    "    StructField(\"vol\", DoubleType(), True),\n",
    "    StructField(\"close_time\", LongType(), True),\n",
    "    StructField(\"quote_asset_vol\", DoubleType(), True),\n",
    "    StructField(\"num_trades\", IntegerType(), True),\n",
    "    StructField(\"taker_base_vol\", DoubleType(), True),\n",
    "    StructField(\"taker_quote_vol\", DoubleType(), True),\n",
    "    StructField(\"ignore\", IntegerType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57444fc8-c16b-425d-9cb2-88c22c9fa9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########################         Read Files from Minio         ########################\n",
    "# Binance\n",
    "bars_df = spark.read.csv(\"s3a://binance-bars/*.csv\", header=True, schema=bars_schema)\n",
    "symbols_df = spark.read.option(\"multiline\", \"true\").option(\"header\", \"true\").json(\"s3a://binance-symbols/*.json\")\n",
    "\n",
    "# Gecko\n",
    "category_df = spark.read.option(\"multiline\", \"true\").option(\"header\", \"true\").json(\"s3a://gecko-category/category.json\")\n",
    "category_details_df = spark.read.option(\"multiline\", \"true\").option(\"header\", \"true\").json(\"s3a://gecko-category-details/category-details.json\")\n",
    "\n",
    "\n",
    "# Create fact and 2 dims\n",
    "order_types_df = symbols_df.withColumn(\"symbols\", explode(col(\"symbols\"))) \\\n",
    "            .select(\"symbols.symbol\", \"symbols.orderTypes\") \\\n",
    "            .withColumn(\"orderTypes\", explode(\"orderTypes\"))\n",
    "\n",
    "distinct_order_types_df = order_types_df.select(\"orderTypes\").distinct() \\\n",
    "    .withColumn(\"order_type_id\", monotonically_increasing_id())\n",
    "\n",
    "distinct_symbols_df = order_types_df.select(\"symbol\").distinct() \\\n",
    "    .withColumn(\"symbol_id\", monotonically_increasing_id())\n",
    "\n",
    "# join and replace with key\n",
    "order_types_df = order_types_df.join(distinct_symbols_df, order_types_df.symbol == distinct_symbols_df.symbol , how = \"left\") \\\n",
    "                            .join(distinct_order_types_df, order_types_df.orderTypes == distinct_order_types_df.orderTypes) \\\n",
    "                            .select(\"symbol_id\", \"order_type_id\")\n",
    "\n",
    "### Create date_dim\n",
    "date_dim_df = spark.createDataFrame(create_date_dim(\"2024-01-01\", \"2024-12-31\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cabf6b1-9c8d-45ce-a0d6-2e60916cb9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########################            Transform          ####################\n",
    "category_df = flatten_category(category_df, spark)\n",
    "category_details_df = flatten_category_details(category_details_df, spark)\n",
    "\n",
    "symbols_df = symbols_df.withColumn(\"symbols\", explode(col(\"symbols\"))) \\\n",
    "                        .select(\n",
    "                            col(\"serverTime\"),\n",
    "                            col(\"symbols.symbol\"),\n",
    "                            col(\"symbols.status\"),\n",
    "                            col(\"symbols.baseAsset\")\n",
    "                        )\n",
    "\n",
    "symbols_df = symbols_df.join(category_details_df, symbols_df.baseAsset == category_details_df.symbol) \\\n",
    "            .withColumnRenamed(\"category_name\", \"categoryName\") \\\n",
    "            .drop(category_details_df[\"name\"], category_details_df[\"url\"], category_details_df[\"symbol\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7739ecf2-2fce-4c3e-9b93-cf82ab191ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1218"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbols_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f95498ff-7f30-4022-b033-2c30af69ee9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bucket if not exists\n",
    "spark_bucket = \"spark-output\"\n",
    "if not minio_client.bucket_exists(spark_bucket):\n",
    "    minio_client.make_bucket(spark_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4624707c-4fe7-4edf-86fb-98c51d7ca034",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################################       Write to Minio        #################\n",
    "# binance\n",
    "bars_df.write \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(f\"s3a://{spark_bucket}/binance.bars\")\n",
    "print(\"Write successuflly binance.bars \")\n",
    "\n",
    "symbols_df.write \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(f\"s3a://{spark_bucket}/binance.symbols\")\n",
    "print(\"Write successuflly binance.symbols \")\n",
    "\n",
    "# gecko\n",
    "category_df.write \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(f\"s3a://{spark_bucket}/gecko.category_overview\")\n",
    "print(\"Write successuflly gecko.category_overview\")\n",
    "\n",
    "# fact and 2 dims\n",
    "order_types_df.write \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(f\"s3a://{spark_bucket}/binance.order_types_fact\")\n",
    "print(\"Write successuflly binance.order_types_fact !\")\n",
    "\n",
    "distinct_order_types_df.write \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(f\"s3a://{spark_bucket}/binance.order_types_dim\")\n",
    "print(\"Write successuflly binance.order_types_dim !\")\n",
    "\n",
    "distinct_symbols_df.write \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(f\"s3a://{spark_bucket}/binance.symbols_dim\")\n",
    "print(\"Write successuflly binance.symbols_dim !\")\n",
    "\n",
    "date_dim_df.write \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(f\"s3a://{spark_bucket}/binance.date_dim\")\n",
    "print(\"Write successuflly binance.date_dim !\")\n",
    "\n",
    "# Finally\n",
    "print(\">> Spark run successfully! << \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12195dd-c952-4674-9879-a056d5f1fb79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3bcd15-1257-45b7-96e7-552c32c51226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62a6ee2-9e3b-46f3-addb-557e20f43ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af108d33-eadc-48a6-b3f4-8cf15bdd8f82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb56917-6372-421c-ab16-16e6aec6e69b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
